{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contemporary-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-slovenia",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20190624140224/cvcss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considerable-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-humanity",
   "metadata": {},
   "source": [
    "### This is markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-abortion",
   "metadata": {},
   "source": [
    "This is markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-prison",
   "metadata": {},
   "source": [
    "### a: insert above cell\n",
    "### b: insert below cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-transaction",
   "metadata": {},
   "source": [
    "# Heading 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-resident",
   "metadata": {},
   "source": [
    "## Heading 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-proportion",
   "metadata": {},
   "source": [
    "### Heading 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-forwarding",
   "metadata": {},
   "source": [
    "*italic*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-relationship",
   "metadata": {},
   "source": [
    "**bold**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-convenience",
   "metadata": {},
   "source": [
    "I am Truong\n",
    "\n",
    "I come from VN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-minneapolis",
   "metadata": {},
   "source": [
    "```\n",
    "I am Truong\n",
    "I come from VN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-incidence",
   "metadata": {},
   "source": [
    "![Python](https://camo.githubusercontent.com/888e388801f947dec7c3d843942c277af25fe2b1aed1821542c4e711f210312a/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f632f63332f507974686f6e2d6c6f676f2d6e6f746578742e7376672f37363870782d507974686f6e2d6c6f676f2d6e6f746578742e7376672e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efficient-roberts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "destroyed-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bright-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Truong!\n"
     ]
    }
   ],
   "source": [
    "print('Hello Truong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-shelter",
   "metadata": {},
   "source": [
    "### 1. Operations, Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afraid-monaco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "essential-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "frank-rogers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-utilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "described-japanese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modulus\n",
    "5 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marked-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "second-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9 ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "animated-showcase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "related-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 // 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convinced-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1j"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "welcome-charge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1+0j)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1j) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-piano",
   "metadata": {},
   "source": [
    "### 2. Data types and data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-nursing",
   "metadata": {},
   "source": [
    "**Programming = data structures + algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-actress",
   "metadata": {},
   "source": [
    "- numeric: integer, float, complex\n",
    "- String\n",
    "- Boolean: True / False\n",
    "- NoneType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "psychological-flooring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "second-puzzle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powered-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complex"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(1 + 2j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "elder-vitamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "general-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('I am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "instrumental-sweet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "published-provision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "through-endorsement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "textile-funeral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True + True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "determined-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True +False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "willing-techno",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Truong'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'I ' + 'am ' + 'Truong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beneficial-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**************************************************'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'*' * 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-frequency",
   "metadata": {},
   "source": [
    "- AND\n",
    "- OR \n",
    "- NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "labeled-pencil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "clean-treat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "distinct-ambassador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "three-triangle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cultural-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "removable-platinum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "automatic-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "understood-bleeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True and False or False or True and not False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-charter",
   "metadata": {},
   "source": [
    "#### 2.1. Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "curious-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "touched-metadata",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "studied-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "related-context",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 != 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-salon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
